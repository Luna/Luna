from Standard.Base import all
import Standard.Base.Data.Base_64.Base_64
import Standard.Base.Errors.Common.Response_Too_Large
import Standard.Base.Errors.File_Error.File_Error
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument
import Standard.Base.Network.HTTP.Request.Request
import Standard.Base.Network.HTTP.Request_Body.Request_Body
import Standard.Base.Network.HTTP.Response.Response
import Standard.Base.Runtime.Context
from Standard.Base.Network.HTTP import http_get_file_sizes_test_only 

from Standard.Table import all
import Standard.Table.Errors.Invalid_JSON_Format

from Standard.Test import all

from enso_dev.Base_Tests.Network.Http.Http_Test_Setup import base_url_with_slash, pending_has_url

import project.Util

polyglot java import org.enso.base.enso_cloud.TransientHTTPResponseCache 

main filter=Nothing =
    suite = Test.build suite_builder->
        add_specs suite_builder
    suite.run_with_filter filter


add_specs suite_builder =
    suite_builder.group "fetching files using HTTP" pending=pending_has_url group_builder->
        group_builder.specify "fetching json" <| Test.with_retries <|
            r = Data.fetch base_url_with_slash+"testfiles/table.json"
            expected_table  = Table.from_rows ["A", "B"] [[1, "x"], [3, "y"]]
            r.to Table . should_equal expected_table

        group_builder.specify "fetching csv" <| Test.with_retries <|
            url = base_url_with_slash+"testfiles/table.csv"
            r = Data.fetch url
            expected_table  = Table.from_rows ["A", "B"] [[1, "x"], [3, "y"]]

            r.should_be_a Table
            r.should_equal expected_table

            r2 = url.to_uri.fetch
            r2.should_be_a Table
            r2.should_equal expected_table

        group_builder.specify "fetching xls" <| Test.with_retries <|
            url = base_url_with_slash+"testfiles/table.xls"
            r = Data.fetch url
            expected_table  = Table.from_rows ["A", "B"] [[1, "x"], [3, "y"]]

            r.should_be_a Excel_Workbook
            r.sheet_names . should_equal ["MyTestSheet"]
            r.read "MyTestSheet" . should_equal expected_table

            r2 = Data.fetch url format=Raw_Response . decode (..Sheet "MyTestSheet")
            r2.should_be_a Table
            r2.should_equal expected_table

        group_builder.specify "fetching xlsx" <| Test.with_retries <|
            url = base_url_with_slash+"testfiles/table.xlsx"
            r = Data.fetch url
            expected_table  = Table.from_rows ["A", "B"] [[1, "x"], [3, "y"]]

            r.should_be_a Excel_Workbook
            r.sheet_names . should_equal ["MyTestSheet"]
            r.read "MyTestSheet" . should_equal expected_table

            r2 = Data.fetch url format=Raw_Response . decode (..Sheet "MyTestSheet")
            r2.should_be_a Table
            r2.should_equal expected_table

            r3 = url.to_uri.fetch format=Raw_Response . decode (..Sheet "MyTestSheet")
            r3.should_be_a Table
            r3.should_equal expected_table

        group_builder.specify "format detection based on Content-Type and Content-Disposition" <| Test.with_retries <|
            content = 'A,B\n1,x\n3,y'
            uri = URI.from (base_url_with_slash+"test_headers")
                . add_query_argument "base64_response_data" (Base_64.encode_text content)
            expected_table = Table.from_rows ["A", "B"] [[1, "x"], [3, "y"]]

            r0 = uri.fetch
            # No automatic parsing, because no content type information is specified.
            r0.should_be_a Response
            r0.content_type . should_equal Nothing
            r0.get_header "Content-Disposition" . should_equal Nothing

            r1 = (uri.add_query_argument "Content-Type" "text/csv").fetch
            r1.should_equal expected_table

            r2 = (uri.add_query_argument "Content-Disposition" 'attachment; filename="my_table.csv"').fetch
            r2.should_equal expected_table

            # If the disposition suggest a text file, we will parse as text:
            r3 = (uri.add_query_argument "Content-Disposition" 'attachment; filename="text.txt"').fetch
            r3.should_be_a Text
            r3.should_equal content

            # Reinterpreting as TSV:
            r4 = (uri.add_query_argument "Content-Type" "text/tab-separated-values").fetch
            r4.should_equal (Table.from_rows ["Column 1"] [["A,B"], ["1,x"], ["3,y"]])

    suite_builder.group "Response caching" pending=pending_has_url group_builder->
        get_num_response_cache_entries =
            TransientHTTPResponseCache.getNumEntries
        with_counts ~action =
            before_count = get_num_response_cache_entries 
            action
            after_count = get_num_response_cache_entries 
            [before_count, after_count]

        reset_size_limits =
            TransientHTTPResponseCache.clearMaxFileSizeOverrideTestOnly
            TransientHTTPResponseCache.clearMaxTotalCacheSizeOverrideTestOnly

        expect_counts expected_counts ~action =
            counts = with_counts action
            counts . should_equal expected_counts frames_to_skip=1

        # Returns true if the cache was used.
        # Does not clear the cache first.
        request_and_get_counts uri method headers cache_policy=Cache_Policy.Default =
            with_counts <|
                response = Data.fetch uri method headers cache_policy=cache_policy
                response.code.is_success . should_be_true

        ##
            # Only use with GET.
            # Clears the cache first.
            with_and_without_caching uri headers =
                Data.clear_response_cache
                request_and_get_counts uri HTTP_Method.Get headers . should_equal [0, 1]
                request_and_get_counts uri HTTP_Method.Get headers . should_equal [1, 1]
                Data.clear_response_cache
                request_and_get_counts uri HTTP_Method.Get headers cache_policy=..Cache_Responses . should_equal [0, 1]
                request_and_get_counts uri HTTP_Method.Get headers cache_policy=..Cache_Responses . should_equal [1, 1]
                Data.clear_response_cache
                request_and_get_counts uri HTTP_Method.Get headers cache_policy=..Dont_Cache_Responses . should_equal [0, 0]
                request_and_get_counts uri HTTP_Method.Get headers cache_policy=..Dont_Cache_Responses . should_equal [0, 0]

        uri0 = base_url_with_slash+'test_download?max-age=16&length=10'
        uri1 = base_url_with_slash+'test_download?max-age=16&length=20'
        uri0_body = "aaaaaaaaaa"
        uri1_body = "aaaaaaaaaaaaaaaaaaaa"
        url_post = base_url_with_slash + "post"
        headers0 = [Header.new "A-Header" "a-header-value", Header.new "A-Header" "a-header-value"]
        headers1 = [Header.new "A-Header" "a-different-header-value", Header.new "A-Header" "a-header-value"]
        # policies = [Cache_Policy.Default, Cache_Policy.Cache_Responses, Cache_Policy.Dont_Cache_Responses]

        group_builder.specify "Cache should return the same repsonse" pending=pending_has_url <| Test.with_retries <|
            Data.clear_response_cache

            HTTP.fetch uri0 . decode_as_text . should_equal uri0_body
            get_num_response_cache_entries
            HTTP.fetch uri0 . decode_as_text . should_equal uri0_body
            HTTP.fetch uri1 . decode_as_text . should_equal uri1_body
            HTTP.fetch uri1 . decode_as_text . should_equal uri1_body
            get_num_response_cache_entries . should_equal 2

            Data.clear_response_cache

            HTTP.fetch uri0 cache_policy=Cache_Policy.Cache_Responses . decode_as_text . should_equal uri0_body
            HTTP.fetch uri0 cache_policy=Cache_Policy.Cache_Responses . decode_as_text . should_equal uri0_body
            HTTP.fetch uri1 cache_policy=Cache_Policy.Cache_Responses . decode_as_text . should_equal uri1_body
            HTTP.fetch uri1 cache_policy=Cache_Policy.Cache_Responses . decode_as_text . should_equal uri1_body
            get_num_response_cache_entries . should_equal 2

            Data.clear_response_cache

            HTTP.fetch uri0 cache_policy=Cache_Policy.Dont_Cache_Responses . decode_as_text . should_equal uri0_body
            HTTP.fetch uri0 cache_policy=Cache_Policy.Dont_Cache_Responses . decode_as_text . should_equal uri0_body
            HTTP.fetch uri1 cache_policy=Cache_Policy.Dont_Cache_Responses . decode_as_text . should_equal uri1_body
            HTTP.fetch uri1 cache_policy=Cache_Policy.Dont_Cache_Responses . decode_as_text . should_equal uri1_body
            get_num_response_cache_entries . should_equal 0

        group_builder.specify "Cache should handle many entries" pending=pending_has_url <| Test.with_retries <|
            count = 20

            Data.clear_response_cache
            0.up_to count . each i->
                HTTP.fetch base_url_with_slash+"test_download?length="+i.to_text . decode_as_text . should_equal ('a' * i)
            get_num_response_cache_entries . should_equal count

            Data.clear_response_cache
            0.up_to count . each i->
                headers = [Header.new "A-Header" "a-header-value-"+i.to_text]
                HTTP.fetch base_url_with_slash+"test_download?length=8" headers=headers . decode_as_text . should_equal ('a' * 8)
            get_num_response_cache_entries . should_equal count

        group_builder.specify "Cache policy should work for HTTP.fetch" pending=pending_has_url <| Test.with_retries <|
            Data.clear_response_cache
            expect_counts [0, 0] <|
                HTTP.fetch uri0 cache_policy=Cache_Policy.Dont_Cache_Responses
                HTTP.fetch uri1 cache_policy=Cache_Policy.Dont_Cache_Responses
            expect_counts [0, 2] <|
                HTTP.fetch uri0 cache_policy=Cache_Policy.Cache_Responses
                HTTP.fetch uri1 cache_policy=Cache_Policy.Cache_Responses
            HTTP.clear_response_cache
            expect_counts [0, 2] <|
                HTTP.fetch uri0
                HTTP.fetch uri1

        group_builder.specify "Cache policy should work for Data.read" pending=pending_has_url <| Test.with_retries <|
            Data.clear_response_cache
            expect_counts [0, 0] <|
                Data.read uri0 cache_policy=Cache_Policy.Dont_Cache_Responses
                Data.read uri1 cache_policy=Cache_Policy.Dont_Cache_Responses
            expect_counts [0, 2] <|
                Data.read uri0 cache_policy=Cache_Policy.Cache_Responses
                Data.read uri1 cache_policy=Cache_Policy.Cache_Responses
            Data.clear_response_cache
            expect_counts [0, 2] <|
                Data.read uri0
                Data.read uri1

        group_builder.specify "Cache policy should work for Data.fetch" pending=pending_has_url <| Test.with_retries <|
            Data.clear_response_cache
            expect_counts [0, 0] <|
                Data.fetch uri0 cache_policy=Cache_Policy.Dont_Cache_Responses
                Data.fetch uri1 cache_policy=Cache_Policy.Dont_Cache_Responses
            expect_counts [0, 2] <|
                Data.fetch uri0 cache_policy=Cache_Policy.Cache_Responses
                Data.fetch uri1 cache_policy=Cache_Policy.Cache_Responses
            Data.clear_response_cache
            expect_counts [0, 2] <|
                Data.fetch uri0
                Data.fetch uri1

        group_builder.specify "Cache policy should work for Data.download" pending=pending_has_url <| Test.with_retries <|
            target_file0 = enso_project.data / "transient" / "my_download0.txt"
            target_file1 = enso_project.data / "transient" / "my_download1.txt"

            Data.clear_response_cache
            expect_counts [0, 0] <|
                target_file0.delete_if_exists
                target_file1.delete_if_exists
                Data.download uri0 target_file0 cache_policy=Cache_Policy.Dont_Cache_Responses
                Data.download uri1 target_file1 cache_policy=Cache_Policy.Dont_Cache_Responses
            expect_counts [0, 2] <|
                target_file0.delete_if_exists
                target_file1.delete_if_exists
                Data.download uri0 target_file0 cache_policy=Cache_Policy.Cache_Responses
                Data.download uri1 target_file1 cache_policy=Cache_Policy.Cache_Responses
            Data.clear_response_cache
            expect_counts [0, 2] <|
                target_file0.delete_if_exists
                target_file1.delete_if_exists
                Data.download uri0 target_file0 
                Data.download uri1 target_file1 

            target_file0.delete_if_exists
            target_file1.delete_if_exists

        group_builder.specify "Should not cache for methods other than GET" pending=pending_has_url <| Test.with_retries <|
            Data.clear_response_cache

            expect_counts [0, 0] <|
                Data.post url_post (Request_Body.Text "hello world")

        group_builder.specify "Should not allow a cache policy for methods other than GET" pending=pending_has_url <| Test.with_retries <|
            request = Request.new HTTP_Method.Post url_post [] Request_Body.Empty
            HTTP.new.request request cache_policy=Cache_Policy.Cache_Responses . should_fail_with Illegal_Argument

        group_builder.specify "Should be able to clear caches" pending=pending_has_url <| Test.with_retries <|
            Data.clear_response_cache
            request_and_get_counts uri0 HTTP_Method.Get headers0 . should_equal [0, 1]
            expect_counts [1, 0] <|
                Data.clear_response_cache

        group_builder.specify "Cache key should only dependon the URI and headers" pending=pending_has_url <| Test.with_retries <|
            Data.clear_response_cache
            expect_counts [0, 1] <|
                Data.fetch uri0
                Data.fetch uri0
            expect_counts [1, 2] <|
                Data.fetch uri1
                Data.fetch uri1

        group_builder.specify "Cache key should depend on the URI" pending=pending_has_url <| Test.with_retries <|
            Data.clear_response_cache
            expect_counts [0, 2] <|
                Data.fetch uri0
                Data.fetch uri1

        group_builder.specify "Cache key should depend on the headers" pending=pending_has_url <| Test.with_retries <|
            Data.clear_response_cache
            expect_counts [0, 2] <|
                Data.fetch uri0 headers=headers0
                Data.fetch uri0 headers=headers1

        # Fetching the trigger uri causes stale entries to be removed.
        fake_now = Date_Time.now
        trigger_uri = base_url_with_slash+'test_download?max-age=10000&length=50'
        set_time_and_get_count advance_secs =
            TransientHTTPResponseCache.setNowOverrideTestOnly (fake_now + (Duration.new seconds=advance_secs))
            Data.fetch trigger_uri
            get_num_response_cache_entries
        fake_time_resetter =
            TransientHTTPResponseCache.setNowOverrideTestOnly Nothing

        group_builder.specify "The cache should expire stale entries" pending=pending_has_url <| Test.with_retries <|
            Data.clear_response_cache

            set_time_and_get_count 0  # Initialize fake now.

            Data.fetch base_url_with_slash+'test_download?max-age=100&length=50'
            Data.fetch base_url_with_slash+'test_download?max-age=200&length=50'
            Data.fetch base_url_with_slash+'test_download?max-age=200&length=51'
            Data.fetch base_url_with_slash+'test_download?max-age=300&length=50'

            Panic.with_finalizer fake_time_resetter <|
                set_time_and_get_count 0 . should_equal 5
                set_time_and_get_count 90 . should_equal 5
                set_time_and_get_count 110 . should_equal 4
                set_time_and_get_count 190 . should_equal 4
                set_time_and_get_count 202 . should_equal 2
                set_time_and_get_count 292 . should_equal 2
                set_time_and_get_count 301 . should_equal 1

        group_builder.specify "The cache should use the Age response header" pending=pending_has_url <| Test.with_retries <|
            Data.clear_response_cache

            set_time_and_get_count 0  # Initialize fake now.

            Data.fetch base_url_with_slash+'test_download?max-age=100&age=50&length=50' # ttl 50
            Data.fetch base_url_with_slash+'test_download?max-age=100&age=30&length=50' # ttl 70
            Data.fetch base_url_with_slash+'test_download?max-age=120&age=50&length=50' # ttl 70
            Data.fetch base_url_with_slash+'test_download?max-age=70&&length=50'        # ttl 70
            Data.fetch base_url_with_slash+'test_download?max-age=160&age=70&length=50' # ttl 90

            Panic.with_finalizer fake_time_resetter <|
                set_time_and_get_count 0 . should_equal 6
                set_time_and_get_count 40 . should_equal 6
                set_time_and_get_count 51 . should_equal 5
                set_time_and_get_count 68 . should_equal 5
                set_time_and_get_count 72 . should_equal 2
                set_time_and_get_count 88 . should_equal 2
                set_time_and_get_count 93 . should_equal 1

        download size = 
            Data.fetch base_url_with_slash+'test_download?length='+size.to_text

        group_builder.specify "Will remove old cache files to keep the total cache size under the total cache size limit" pending=pending_has_url <| Test.with_retries <|
            Panic.with_finalizer reset_size_limits <|
                reset_size_limits
                TransientHTTPResponseCache.setMaxTotalCacheSizeOverrideTestOnly 100

                download 30
                download 50
                download 10
                http_get_file_sizes_test_only . should_equal [10, 30, 50]
                download 20
                http_get_file_sizes_test_only . should_equal [10, 20, 50]
                download 40
                http_get_file_sizes_test_only . should_equal [10, 20, 40]
                download 35
                http_get_file_sizes_test_only . should_equal [20, 35, 40]

        group_builder.specify "Will remove old cache files based on how recently they were used" pending=pending_has_url <| Test.with_retries <|
            Panic.with_finalizer reset_size_limits <|
                reset_size_limits
                TransientHTTPResponseCache.setMaxTotalCacheSizeOverrideTestOnly 100

                download 30
                download 50
                download 10
                http_get_file_sizes_test_only . should_equal [10, 30, 50]
                # Use 30 again so it's considered more recently used.
                download 30
                http_get_file_sizes_test_only . should_equal [10, 30, 50]
                download 20
                http_get_file_sizes_test_only . should_equal [10, 20, 30]
                download 45
                http_get_file_sizes_test_only . should_equal [20, 30, 45]

        group_builder.specify "Will not download or cache a file with a content length greater than the single file limit" pending=pending_has_url <| Test.with_retries <|
            Panic.with_finalizer reset_size_limits <|
                reset_size_limits
                TransientHTTPResponseCache.setMaxFileSizeOverrideTestOnly 100
                err = download 110 . catch
                err.should_be_a Response_Too_Large
                err.size . should_equal 110
                err.limit . should_equal 100

        ##
            group_builder.specify "Will not download or cache a file without a content length, but which is greater than the single file limit" pending=pending_has_url <| Test.with_retries <|
            group_builder.specify "Deleted cache files that have an open input stream connected to them are still readable" pending=pending_has_url <| Test.with_retries <|
            group_builder.specify "ResponseTooLargeException does not contain any secrets" pending=pending_has_url <| Test.with_retries <|

            group_builder.specify "Should not cache if the request fails" pending=pending_has_url <| Test.with_retries <|
            group_builder.specify "Should work with secrets in the URI" pending=pending_has_url <| Test.with_retries <|
            group_builder.specify "Should work with secrets in the headers" pending=pending_has_url <| Test.with_retries <|
            group_builder.specify "Cache state should be consistent" pending=pending_has_url <| Test.with_retries <|
                # Check that map matches files on disk
