from Standard.Base import all
from Standard.Base.Random import random_uuid
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument

import Standard.Table.Data.Table.Table as In_Memory_Table
from Standard.Table import Aggregate_Column
from Standard.Table.Errors import all

import project.Connection.Connection.Connection
import project.Data.SQL_Query.SQL_Query
import project.Data.SQL_Statement.SQL_Statement
import project.Data.Table.Table as Database_Table
import project.Data.Update_Action.Update_Action
import project.Internal.In_Transaction.In_Transaction
import project.Internal.IR.Query.Query
import project.Internal.IR.SQL_Expression.SQL_Expression
from project.Errors import all

## PRIVATE
   A helper that can upload a table from any backend to a database.
   It should be run within a transaction.
upload_table source_table connection table_name primary_key temporary structure_only on_problems =
    case source_table of
        _ : In_Memory_Table ->
            upload_in_memory_table source_table connection table_name primary_key temporary structure_only on_problems
        _ : Database_Table ->
            upload_database_table source_table connection table_name primary_key temporary structure_only on_problems
        _ ->
            Panic.throw <| Illegal_Argument.Error ("Unsupported table type: " + Meta.get_qualified_type_name source_table)

## PRIVATE
   It should be run within a transaction.
upload_in_memory_table source_table connection table_name primary_key temporary structure_only on_problems =
    In_Transaction.ensure_in_transaction <|
        resolved_primary_key = resolve_primary_key self primary_key
        effective_table_name = resolve_effective_table_name table_name temporary
        create_table_statement = prepare_create_table_statement connection effective_table_name self.columns resolved_primary_key temporary on_problems

        ## `create_table_statement.if_not_error` is used to ensure that if there are
           any dataflow errors up to this point, we want to propagate them and not
           continue. Otherwise, they could 'leak' to `Panic.rethrow` and be wrongly
           raised as panics.
        upload_status = create_table_statement.if_not_error <|
            translate_known_upload_errors self connection resolved_primary_key <|
                Panic.rethrow <| connection.execute_update create_table_statement
                if structure_only.not then
                    insert_template = make_batched_insert_template connection effective_table_name self.column_names
                    statement_setter = connection.dialect.get_statement_setter
                    Panic.rethrow <| connection.jdbc_connection.batch_insert insert_template statement_setter self default_batch_size

        upload_status.if_not_error <|
            connection.query (SQL_Query.Table_Name effective_table_name)

## PRIVATE
   It should be run within a transaction.
upload_database_table source_table connection table_name primary_key temporary structure_only on_problems =
    In_Transaction.ensure_in_transaction <|
        resolved_primary_key = resolve_primary_key self primary_key
        effective_table_name = resolve_effective_table_name table_name temporary
        create_table_statement = prepare_create_table_statement connection effective_table_name self.columns resolved_primary_key temporary on_problems
        connection_check = if self.connection.jdbc_connection == connection.jdbc_connection then True else
            Error.throw (Unsupported_Database_Operation.Error "The Database table to be uploaded must be coming from the same connection as the connection on which the new table is being created. Cross-connection uploads are currently not supported. To work around this, you can first `.read` the table into memory and then upload it from memory to a different connection.")

        upload_status = connection_check.if_not_error <| create_table_statement.if_not_error <|
            translate_known_upload_errors self connection resolved_primary_key <|
                Panic.rethrow <| connection.execute_update create_table_statement
                if structure_only.not then
                    ## We need to ensure that the columns in this statement are matching
                       positionally the columns in `create_table_statement`. But we
                       create both from the same table, so that is guaranteed.
                    copy_into_statement = connection.dialect.generate_sql <|
                        Query.Insert_From_Select effective_table_name self.to_select_query
                    Panic.rethrow <| connection.execute_update copy_into_statement

        upload_status.if_not_error <|
            connection.query (SQL_Query.Table_Name effective_table_name)

## PRIVATE
   Ensures that provided primary key columns are present in the table and that
   there are no duplicates.
resolve_primary_key table primary_key = case primary_key of
    Nothing -> Nothing
    _ : Vector -> if primary_key.is_empty then Nothing else
        table.select_columns primary_key reorder=True . column_names

## PRIVATE
   Inspects any `SQL_Error` thrown and replaces it with a more precise error
   type when available.
translate_known_upload_errors source_table connection primary_key ~action =
    handler caught_panic =
        error_mapper = connection.dialect.get_error_mapper
        sql_error = caught_panic.payload
        case error_mapper.is_primary_key_violation sql_error of
            True -> raise_duplicated_primary_key_error source_table primary_key caught_panic
            False -> Panic.throw caught_panic
    Panic.catch SQL_Error action handler

## PRIVATE
   Creates a `Non_Unique_Primary_Key` error containing information about an
   example group violating the uniqueness constraint.
raise_duplicated_primary_key_error source_table primary_key original_panic =
    agg = source_table.aggregate [Aggregate_Column.Count]+(primary_key.map Aggregate_Column.Group_By)
    filtered = agg.filter column=0 (Filter_Condition.Greater than=1)
    materialized = filtered.read max_rows=1
    case materialized.row_count == 0 of
        ## If we couldn't find a duplicated key, we give up the translation and
           rethrow the original panic containing the SQL error. This could
           happen if the constraint violation is on some non-trivial key, like
           case insensitive.
        True -> Panic.throw original_panic
        False ->
            row = materialized.first_row.to_vector
            example_count = row.first
            example_entry = row.drop 1
            Error.throw (Non_Unique_Primary_Key.Error primary_key example_entry example_count)


## PRIVATE
   Creates a statement that will create a table with structure determined by the
   provided columns.

   The `primary_key` columns must be present in `columns`, but it is the
   responsibility of the caller to ensure that, otherwise the generated
   statement will be invalid.
prepare_create_table_statement : Connection -> Text -> Vector -> Vector Text -> Boolean -> Problem_Behavior -> SQL_Statement
prepare_create_table_statement connection table_name columns primary_key temporary on_problems =
    type_mapping = connection.dialect.get_type_mapping
    column_descriptors = columns.map column->
        name = column.name
        value_type = column.value_type
        sql_type = type_mapping.value_type_to_sql value_type on_problems
        sql_type_text = type_mapping.sql_type_to_text sql_type
        Pair.new name sql_type_text
    connection.dialect.generate_sql <|
        Query.Create_Table table_name column_descriptors primary_key temporary

## PRIVATE
   Generates a random table name if it was nothing, if it is allowed (temporary=True).
resolve_effective_table_name table_name temporary = case table_name of
    Nothing -> if temporary then "temporary-table-"+random_uuid else
        Error.throw (Illegal_Argument.Error "A name must be provided when creating a non-temporary table.")
    _ : Text -> table_name

## PRIVATE
   The recommended batch size seems to be between 50 and 100.
   See: https://docs.oracle.com/cd/E18283_01/java.112/e16548/oraperf.htm#:~:text=batch%20sizes%20in%20the%20general%20range%20of%2050%20to%20100
default_batch_size = 100

## PRIVATE
make_batched_insert_template : Connection -> Text -> Vector (Vector Text) -> SQL_Query
make_batched_insert_template connection table_name column_names =
    # We add Nothing as placeholders, they will be replaced with the actual values later.
    pairs = column_names.map name->[name, SQL_Expression.Constant Nothing]
    query = connection.dialect.generate_sql <| Query.Insert table_name pairs
    template = query.prepare.first
    template

## PRIVATE
common_update_table source_table connection table_name update_action key_columns error_on_missing_columns on_problems =
    Panic.recover SQL_Error <|
        connection.jdbc_connection.run_within_transaction <|
            target_table = connection.query (SQL_Query.Table_Name table_name)
            # We catch the `Table_Not_Found` error and handle it specially, if the error was different, it will just get passed through further.
            handle_error = target_table.catch Table_Not_Found error->
                # Rethrow the error with more info.
                msg_suffix = " Use `Connection.create_table` to create a table before trying to append to it."
                new_error = error.with_changed_extra_message msg_suffix
                Error.throw new_error
            if target_table.is_error then handle_error else
                tmp_table_name = "temporary-source-table-"+random_uuid
                tmp_table = upload_table source_table connection tmp_table_name key_columns temporary=True structure_only=False on_problems=Problem_Behavior.Report_Error
                tmp_table.if_not_error <|
                    resulting_table = append_to_existing_table
                    connection.drop_table tmp_table.name
                    resulting_table

## PRIVATE
append_to_existing_table source_table target_table update_action key_columns error_on_missing_columns =
    source_columns = Set.from_vector source_table.column_names
    target_columns = Set.from_vector target_table.column_names
    extra_columns = source_columns - target_columns
    if extra_columns.not_empty then Error.throw (Unmatched_Columns.Error extra_columns) else
        missing_columns = target_columns - source_columns
        if missing_columns.not_empty && error_on_missing_columns then Error.throw (Missing_Input_Columns.Error missing_columns "the source table") else
            Error.throw "TODO: Implement the actual update logic."
